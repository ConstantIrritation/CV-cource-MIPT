{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4aa41351-5381-49d2-af4e-4c484f651b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47d841fb-d032-4869-81d4-03f4bda4360a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "958e0f22-980e-440f-a480-5d48371bb5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63e20b11-1aff-46bc-91bf-ff26807f7186",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms.v2 as T\n",
    "from torch import nn\n",
    "import cv2\n",
    "from pylab import imshow\n",
    "import torch.nn.functional as F\n",
    "from numpy import array\n",
    "import random\n",
    "\n",
    "import lightning as L\n",
    "import torchmetrics\n",
    "\n",
    "from torch.utils import data\n",
    "import PIL.Image\n",
    "import albumentations as A\n",
    "import albumentations.pytorch.transforms\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b44ef4a-5c58-491d-b036-61db5d0da179",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0fd9cf1-1eb3-454c-827e-1132bc1058ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_path = Path().absolute() / 'public_tests' / '00_test_img_input' / 'train' / 'images'\n",
    "train_gt_path = Path().absolute() / 'public_tests' / '00_test_img_input' / 'train' / 'gt.csv'\n",
    "assert train_images_path.exists(), train_images_path.absolute()\n",
    "assert train_gt_path.exists(), train_gt_path.absolute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b154956e-a491-4a2d-8491-05da78b57f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(filename):\n",
    "    res = {}\n",
    "    with open(filename) as fhandle:\n",
    "        next(fhandle)\n",
    "        for line in fhandle:\n",
    "            filename, class_id = line.rstrip('\\n').split(',')\n",
    "            res[filename] = int(class_id)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45562325-9916-450e-82a1-f43790becef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gt = read_csv(train_gt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09315ad6-1b1d-44d6-a5f7-47e9d68615a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCustomDataset(data.Dataset):\n",
    "    def __init__(self, mode, root_dir=train_images_path,\n",
    "                 train_fraction=0.9, split_seed=42, transform=None,):\n",
    "\n",
    "        paths = []\n",
    "        labels = []\n",
    "\n",
    "        tr, val = train_test_split(list(train_gt.items()),\n",
    "                                   train_size=train_fraction,\n",
    "                                   random_state=split_seed,\n",
    "                                   stratify=list(train_gt.values()))\n",
    "    \n",
    "        if mode == \"train\":\n",
    "            cls = tr\n",
    "        elif mode == \"valid\":\n",
    "            cls = val\n",
    "        else:\n",
    "            raise RuntimeError(f\"Invalid mode: {mode!r}\")\n",
    "            \n",
    "        for obj, label in cls:\n",
    "            paths.append(os.path.join(root_dir, obj))\n",
    "            labels.append(label)\n",
    "\n",
    "        self._paths = np.array(paths)\n",
    "        self._labels = np.array(labels)\n",
    "\n",
    "        self._transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self._paths[index]\n",
    "        label = self._labels[index]\n",
    "\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        tr = self._transform(image=image)\n",
    "        image = tr[\"image\"]\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "30f6e6c6-0a25-45bd-9fc6-6184fb2b48b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "NETWORK_SIZE = (224, 224)\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "common_transforms = [\n",
    "    A.Resize(*NETWORK_SIZE),\n",
    "    A.ToFloat(max_value=255),\n",
    "    A.Normalize(max_pixel_value=1.0, mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "    A.pytorch.transforms.ToTensorV2(),\n",
    "]\n",
    "\n",
    "aug_transforms = [\n",
    "    A.OneOf([\n",
    "        A.HorizontalFlip(),\n",
    "        A.Rotate(border_mode=cv2.BORDER_CONSTANT),\n",
    "    ], p=0.5),\n",
    "    A.OneOf([\n",
    "        A.RGBShift(),\n",
    "        A.ToGray(),\n",
    "        A.RandomBrightnessContrast(),\n",
    "        A.Blur(blur_limit=3),\n",
    "        A.CLAHE(),\n",
    "        A.HueSaturationValue(),\n",
    "    ], p=0.5)\n",
    "]\n",
    "\n",
    "MyTransformTrain = A.Compose(aug_transforms + common_transforms)\n",
    "MyTransformVal = A.Compose(common_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0baa5cb6-f7be-4563-93bc-1f6eddb730ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = MyCustomDataset(mode=\"train\", transform=MyTransformTrain)\n",
    "ds_valid = MyCustomDataset(mode=\"valid\", transform=MyTransformVal)\n",
    "\n",
    "dl_train = data.DataLoader(\n",
    "    ds_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=os.cpu_count(),\n",
    ")\n",
    "dl_valid = data.DataLoader(\n",
    "    ds_valid,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=os.cpu_count(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "70c1a584-ff24-4e0d-85da-e5fcabe440d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frozen_model(num_classes, unfreeze_layers, transfer=True):\n",
    "    weights = torchvision.models.MobileNet_V2_Weights.DEFAULT if transfer else None\n",
    "    model = torchvision.models.mobilenet_v2(weights=weights)\n",
    "\n",
    "    model.classifier = nn.Sequential(\n",
    "        # nn.AdaptiveAvgPool2d(1280),\n",
    "        nn.Linear(model.last_channel, 512),\n",
    "        nn.BatchNorm1d(512),\n",
    "        nn.ReLU6(inplace=True),\n",
    "        nn.Dropout(p=0.2),\n",
    "        nn.Linear(512, num_classes)\n",
    "        # nn.Softmax(dim=1),\n",
    "    )\n",
    "\n",
    "    for child in list(model.features.children())[:-unfreeze_layers]:\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e2ff8c29-c070-4ff0-a029-17e0a6905da9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get_frozen_model(50, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ca2e46a1-b3c7-4724-a0bc-ccb0391d3638",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from torchsummary import summary\n",
    "# summary(get_frozen_model(50, 3).to(DEVICE), (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "73667f44-27cd-40a2-9715-e5f80701ca04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTrainingModuleFrozen(L.LightningModule):\n",
    "    def __init__(self, num_classes, unfrozen_layers):\n",
    "        super().__init__()\n",
    "        self.model = get_frozen_model(num_classes, unfrozen_layers)\n",
    "        self.train_loss = []\n",
    "        self.valid_accs = []\n",
    "        self.accuracy = torchmetrics.classification.Accuracy(\n",
    "            task=\"multiclass\",\n",
    "            num_classes=num_classes,\n",
    "        )\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=1e-3)\n",
    "        lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3)\n",
    "        lr_scheduler_config = {\n",
    "            \"scheduler\": lr_scheduler,\n",
    "            \"interval\": \"epoch\",\n",
    "            \"monitor\": \"valid_accs\",\n",
    "        }\n",
    "\n",
    "        return [optimizer], [lr_scheduler_config]\n",
    "  \n",
    "    def training_step(self, batch):\n",
    "        return self._step(batch, \"train\")\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        return self._step(batch, \"valid\")\n",
    "\n",
    "    def _step(self, batch, kind):\n",
    "        x, y = batch\n",
    "        p = self.model(x)\n",
    "        loss = F.cross_entropy(p, y)\n",
    "        accs = self.accuracy(p.argmax(axis=-1), y)\n",
    "\n",
    "        metrics = {\n",
    "            f\"{kind}_accs\": accs,\n",
    "            f\"{kind}_loss\": loss,\n",
    "        }\n",
    "        self.log_dict(\n",
    "            metrics,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "            on_step=kind == \"train\",\n",
    "            on_epoch=True,\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "def my_early_stopping():\n",
    "    # Monitor a metric and stop training when it stops improving.\n",
    "    return L.pytorch.callbacks.EarlyStopping(\n",
    "        monitor=\"valid_accs\",\n",
    "        mode=\"max\",\n",
    "        patience=5,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "\n",
    "def my_training_checkpoint():\n",
    "    # Save the training module periodically by monitoring a quantity.\n",
    "    return L.pytorch.callbacks.ModelCheckpoint(\n",
    "        filename=\"{epoch}-{valid_accs:.3f}\",\n",
    "        monitor=\"valid_accs\",\n",
    "        mode=\"max\",\n",
    "        save_top_k=1,\n",
    "        save_last=True,\n",
    "    )\n",
    "\n",
    "\n",
    "def my_lr_monitor():\n",
    "    return L.pytorch.callbacks.LearningRateMonitor()\n",
    "\n",
    "\n",
    "def my_progress_bar():\n",
    "    return L.pytorch.callbacks.TQDMProgressBar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cb9828d5-673e-49a6-a8f8-4e78fe2cd6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1,4]\n",
      "\n",
      "  | Name     | Type               | Params | Mode \n",
      "--------------------------------------------------------\n",
      "0 | model    | MobileNetV2        | 2.9 M  | train\n",
      "1 | accuracy | MulticlassAccuracy | 0      | train\n",
      "--------------------------------------------------------\n",
      "1.9 M     Trainable params\n",
      "1.0 M     Non-trainable params\n",
      "2.9 M     Total params\n",
      "11.626    Total estimated model params size (MB)\n",
      "217       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                                            …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/krotovan/.local/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (17) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "085a012d4749408388bc7faa5106378e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric valid_accs improved. New best score: 0.636\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric valid_accs improved by 0.100 >= min_delta = 0.0. New best score: 0.736\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric valid_accs improved by 0.044 >= min_delta = 0.0. New best score: 0.780\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric valid_accs did not improve in the last 5 records. Best score: 0.780. Signaling Trainer to stop.\n"
     ]
    }
   ],
   "source": [
    "training_module = MyTrainingModuleFrozen(50, 3)\n",
    "trainer = L.Trainer(accelerator=\"auto\",\n",
    "                    max_epochs=30,\n",
    "                    callbacks=[\n",
    "                        my_progress_bar(),\n",
    "                        my_lr_monitor(),\n",
    "                        my_training_checkpoint(),\n",
    "                        my_early_stopping(),\n",
    "                    ],)\n",
    "trainer.fit(training_module, dl_train, dl_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "95bb43a1-245b-4e62-aee7-21de4cac80b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = MyTrainingModuleFrozen.load_from_checkpoint(\"lightning_logs/version_45/checkpoints/epoch=23-valid_accs=0.800.ckpt\", num_classes=50, unfrozen_layers=3).model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4116bfa5-72fa-43fe-9952-fe1cc9e7be71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = training_module.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c633506c-8a70-4212-89a3-8fdb72ccbc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"birds_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "beadaed9-2fd9-4465-8411-fca9ccf75067",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images_path = Path().absolute() / 'public_tests' / '00_test_img_input' / 'test' / 'images'\n",
    "test_gt_path = Path().absolute() / 'public_tests' / '00_test_img_gt' / 'gt.csv'\n",
    "assert test_images_path.exists(), test_images_path.absolute()\n",
    "assert test_gt_path.exists(), test_gt_path.absolute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "49ef7637-e784-4e9f-ad8d-acce92179791",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCustomTestDataset(data.Dataset):\n",
    "    def __init__(self, root_dir=test_images_path,\n",
    "                 transform=None):\n",
    "\n",
    "        paths = []\n",
    "        cls_paths = sorted(glob.glob(f\"{root_dir}/*\"))\n",
    "        paths.extend(cls_paths)\n",
    "\n",
    "        self._len = len(paths)\n",
    "        self._paths = paths\n",
    "        if transform is None:\n",
    "            transform = DEFAULT_TRANSFORM\n",
    "        self._transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self._paths[index]\n",
    "\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        image = self._transform(image=image)[\"image\"]\n",
    "\n",
    "        return image, img_path.split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "af0c7ead-1487-45ba-9ddc-cf22f296f3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test = MyCustomTestDataset(transform=MyTransformVal)\n",
    "\n",
    "dl_test = data.DataLoader(\n",
    "    ds_test,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=os.cpu_count(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1990b15f-f3d6-4f71-b3a2-46f98f952b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(fitted_model):\n",
    "    preds = {}\n",
    "    model = fitted_model\n",
    "    model.eval()\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    progress_test = tqdm(\n",
    "        total=len(dl_test),\n",
    "        leave=True,\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        for x_batch, x_path in dl_test:\n",
    "            x_batch = x_batch.to(DEVICE)\n",
    "            \n",
    "            prediction = model(x_batch)\n",
    "            for i in range(len(x_batch)):\n",
    "                prediction_val = prediction[i].cpu().detach().numpy()\n",
    "                preds[x_path[i]] = prediction_val.argmax(axis=-1)\n",
    "\n",
    "            progress_test.update()\n",
    "        progress_test.close()\n",
    "    return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b407165d-8abc-4a77-b94a-863fbaea4606",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_csv(img_classes, filename):\n",
    "    with open(filename, 'w') as fhandle:\n",
    "        print('filename,class_id', file=fhandle)\n",
    "        for filename in sorted(img_classes.keys()):\n",
    "            print('%s,%d' % (filename, img_classes[filename]), file=fhandle)\n",
    "\n",
    "\n",
    "def check_test():\n",
    "    output = read_csv('output.csv')\n",
    "    gt = read_csv(test_gt_path)\n",
    "\n",
    "    correct = 0\n",
    "    total = len(gt)\n",
    "    for k, v in gt.items():\n",
    "        if output[k] == v:\n",
    "            correct += 1\n",
    "\n",
    "    accuracy = correct / total\n",
    "\n",
    "    res = 'Ok, accuracy %.4f' % accuracy\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ea24ab-8beb-4e45-b644-e16678400d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_classes = test(model)\n",
    "save_csv(img_classes, Path().absolute() / 'output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "58d54470-dc1f-4089-9708-b7e265358979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok, accuracy 0.9724\n"
     ]
    }
   ],
   "source": [
    "check_test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
